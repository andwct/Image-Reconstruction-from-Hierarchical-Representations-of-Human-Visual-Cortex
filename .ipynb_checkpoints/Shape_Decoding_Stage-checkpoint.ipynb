{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from model import shape_decoding_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from model.models import *\n",
    "from data.datasets import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2521)\n",
      "(1000, 2521)\n",
      "(6000, 256, 256)\n",
      "(1000, 256, 256)\n",
      "Training shape decoder:\n"
     ]
    }
   ],
   "source": [
    "fmri = torch.load('./extracted_fmri/sub_1_visual_cortex_training_ROI_V1.pt')\n",
    "train_fmri = fmri[0:6000]\n",
    "test_fmri = fmri[5000:6000]\n",
    "print(train_fmri.shape)\n",
    "print(test_fmri.shape)\n",
    "# train_imgs = torch.load(\"train_np_images_resized256_saliency_new.pt\")\n",
    "# print(train_imgs.shape)\n",
    "# test_imgs = torch.load(\"test_np_images_resized256_saliency_new.pt\")\n",
    "# print(test_imgs.shape)\n",
    "\n",
    "# imgs = torch.load(\"./data/images/images/resized256_matted.pt\")\n",
    "# imgs = torch.load(\"./data/images/images/resized256_color_matted_train_6000.pt\")\n",
    "imgs = torch.load(\"./data/images/images/resized256_color_matted_grayscale_blurred_train_6000.pt\")\n",
    "train_imgs = imgs[0:6000]\n",
    "test_imgs = imgs[5000:6000]\n",
    "print(train_imgs.shape)\n",
    "print(test_imgs.shape)\n",
    "print('Training shape decoder:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #2521 ->16384 --> 65536\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, fmri_size, latent_dim):\n",
    "#         super(Decoder, self).__init__()\n",
    "        \n",
    "        \n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(fmri_size, latent_dim),\n",
    "#             nn.Tanh(),\n",
    "\n",
    "#         )\n",
    "\n",
    "#     def forward(self, img):\n",
    "# #         print(\"img.shape\")\n",
    "# #         print(img.shape)\n",
    "#         img_flat = img.view(img.shape[0], -1)\n",
    "# #         print(\"img_flat.shape\")\n",
    "# #         print(img_flat.shape)\n",
    "#         x = self.model(img_flat)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2521 ->16384 --> 65536\n",
    "class Decoder2(nn.Module):\n",
    "    def __init__(self, fmri_size, latent_dim):\n",
    "        super(Decoder2, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(fmri_size, 55*55),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(55*55, latent_dim),\n",
    "            nn.Tanh(),\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "#         print(\"img.shape\")\n",
    "#         print(img.shape)\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "#         print(\"img_flat.shape\")\n",
    "#         print(img_flat.shape)\n",
    "        x = self.model(img_flat)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=0.5\n",
    "b2=0.999\n",
    "output_size = 256\n",
    "n_epochs = 300\n",
    "batch_size = 60\n",
    "lr=0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "latent_dim = output_size*output_size\n",
    "\n",
    "total_blocks = fmri.shape[0]\n",
    "fmri_size = fmri.shape[1]\n",
    "print(fmri_size)\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "# Use L1Loss() as pixelwise_loss\n",
    "\n",
    "pixelwise_loss = torch.nn.L1Loss()\n",
    "val_e_loss_orig = float(\"inf\")\n",
    "# Initialize generator and discriminator\n",
    "decoder = Decoder2(fmri_size, latent_dim)\n",
    "# decoder = Decoder_conv(fmri_size)\n",
    "\n",
    "if cuda:\n",
    "    decoder.cuda()\n",
    "    pixelwise_loss.cuda()\n",
    "\n",
    "\n",
    "\n",
    "optimizer_E = torch.optim.Adam(decoder.parameters(), lr=lr, betas=(b1, b2))\n",
    "scheduler_E = torch.optim.lr_scheduler.StepLR(optimizer_E, step_size=5, gamma=0.7, last_epoch=-1)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "loss_vec = []\n",
    "train_fmri = torch.from_numpy(train_fmri)\n",
    "train_fmri = train_fmri.type(Tensor)\n",
    "test_fmri = torch.from_numpy(test_fmri)\n",
    "test_fmri = test_fmri.type(Tensor)\n",
    "train_imgs = torch.from_numpy(train_imgs)\n",
    "train_imgs = train_imgs.type(Tensor)\n",
    "test_imgs = torch.from_numpy(test_imgs)\n",
    "test_imgs = test_imgs.type(Tensor)\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    decoder.train()\n",
    "    \n",
    "\n",
    "#     imgs = torch.from_numpy(imgs)\n",
    "#     imgs = imgs.type(Tensor)\n",
    "    for i in range(0, batch_size):\n",
    "        fmri_data = train_fmri[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "        real_imgs = train_imgs[i * batch_size:(i + 1) * batch_size]\n",
    "#         print(fmri_data.shape)\n",
    "        latent_vector = decoder(fmri_data)\n",
    "#         print(\"latent_vector\")\n",
    "#         print(latent_vector.shape)\n",
    "        latent_vector = latent_vector.cuda()\n",
    "        obj_vector = real_imgs.reshape(real_imgs.shape[0], -1)\n",
    "    \n",
    "#         print(\"obj_vector\")\n",
    "#         print(obj_vector.shape)\n",
    "\n",
    "#         print(latent_vector.shape)\n",
    "#         print(\"latent_vector\")\n",
    "#         print(latent_vector.shape)\n",
    "        \n",
    "        e_loss = pixelwise_loss(obj_vector, latent_vector)\n",
    "        optimizer_E.zero_grad()\n",
    "        e_loss.backward()\n",
    "        loss_vec.append(e_loss.item())\n",
    "        with torch.no_grad():\n",
    "            optimizer_E.step()\n",
    "\n",
    "    #         del(fmri_data)\n",
    "    #         torch.cuda.empty_cache()\n",
    "    scheduler_E.step()\n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            decoder.eval()\n",
    "            val_fmri_data = test_fmri\n",
    "            val_real_imgs = test_imgs\n",
    "            latent_v = decoder(val_fmri_data)\n",
    "            obj_vector = val_real_imgs.reshape(val_real_imgs.shape[0], -1)\n",
    "#             print(latent_v.shape)\n",
    "#             print(obj_vector.shape)\n",
    "            val_e_loss = pixelwise_loss(obj_vector, latent_v)\n",
    "            if val_e_loss < val_e_loss_orig:\n",
    "                print(\"epoch\"+str(epoch))\n",
    "                print(val_e_loss)\n",
    "                torch.save(decoder.state_dict(), \"./shape_decoder/\"+ timestr+\"_\"+str(epoch)+\"_\"+str(val_e_loss.item())+\".pt\")\n",
    "                val_e_loss_orig = val_e_loss\n",
    "                \n",
    "torch.save(decoder.state_dict(), \"./shape_decoder/\"+ timestr+\"_\"+str(epoch)+\"_decoder2_\"+\"final_v1\"+\".pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     fmri = torch.from_numpy(fmri)\n",
    "#     fmri = fmri.type(Tensor)\n",
    "#     imgs = decoder(fmri)\n",
    "#     imgs = imgs.view(fmri.shape[0], output_size, output_size, 3)\n",
    "#     imgs = imgs.data.cpu() * 255.0\n",
    "#     imgs = np.asarray(imgs)\n",
    "# plt.figure()\n",
    "# plt.title('Loss')\n",
    "# plt.plot(loss_vec)\n",
    "# plt.xlabel('Epoch num')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show() \n",
    "print(\"done\")\n",
    "# save_demo1_images(imgs, raw_imgs)\n",
    "#     return decoder, imgs, raw_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(fmri)\n",
    "del(decoder)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6000, 2521])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder2(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=2521, out_features=3025, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=3025, out_features=65536, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "# fmri = torch.load('./extracted_fmri/sub_1_visual_cortex_testing_ROI_V2.pt')\n",
    "fmri = torch.load('./extracted_fmri/sub_1_visual_cortex_training_ROI_V1.pt')\n",
    "fmri = torch.from_numpy(fmri)\n",
    "fmri = fmri.type(Tensor)\n",
    "print(fmri.shape)\n",
    "# decoder = Decoder_conv(fmri.shape[1],256*256)\n",
    "decoder = Decoder2(fmri.shape[1], 256*256)\n",
    "# decoder = Decoder_conv(fmri.shape[1])\n",
    "decoder.load_state_dict(torch.load(\"./shape_decoder/20210716-094128_299_decoder2_final_v1.pt\"))\n",
    "decoder.eval()\n",
    "fmri = fmri.to('cuda:0') # needs assignment\n",
    "decoder.to('cuda:0') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = decoder(fmri)\n",
    "\n",
    "imgs = imgs.view(fmri.shape[0], 256, 256)\n",
    "imgs = imgs.data.cpu() * 255.0\n",
    "imgs = np.asarray(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "train_image_list = torch.load(\"./data/images/images/train_image_list.pt\")\n",
    "print(len(train_image_list))\n",
    "test_image_list = torch.load(\"./data/images/images/test_image_list.pt\")\n",
    "print(len(test_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# name = \"Shape_prediction_Decoder_conv_sub1_V3_0713/\"\n",
    "# directory_name = \"./data/images/images/\"+name\n",
    "\n",
    "# if not os.path.exists(directory_name + \"train_6000\"):\n",
    "#     os.makedirs(directory_name + \"train_6000\")\n",
    "# if not os.path.exists(directory_name + \"test_1200\"):\n",
    "#     os.makedirs(directory_name + \"test_1200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(train_image_list)):\n",
    "    num_0 = 4 - len(str(i))\n",
    "    new = \"0\"*num_0 + str(i)+\".jpg\"\n",
    "    sp = Image.fromarray(imgs[i])\n",
    "\n",
    "    raw = cv2.imread(\"./data/images/images/training/\"+train_image_list[i])\n",
    "    raw  = cv2.resize(np.array(raw), dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "    raw = cv2.cvtColor(raw, cv2.COLOR_BGR2RGB)\n",
    "    raw = Image.fromarray(raw)\n",
    "    target = Image.new('RGB', (256 * 2, 256))\n",
    "    target.paste(raw, (0, 0, 256, 256))\n",
    "    target.paste(sp, (256, 0, 512, 256))\n",
    "\n",
    "    plt.imshow(target)\n",
    "    plt.show()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 2521])\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "# fmri = torch.load('./extracted_fmri/sub_1_visual_cortex_testing_ROI_V2.pt')\n",
    "fmri = torch.load('./extracted_fmri/sub_1_visual_cortex_testing_ROI_V1.pt')\n",
    "fmri = torch.from_numpy(fmri)\n",
    "fmri = fmri.type(Tensor)\n",
    "print(fmri.shape)\n",
    "decoder = Decoder2(fmri.shape[1], 256*256)\n",
    "# decoder = Decoder_conv(fmri.shape[1])\n",
    "decoder.load_state_dict(torch.load(\"./shape_decoder/20210716-094128_299_decoder2_final_v1.pt\"))\n",
    "decoder.eval()\n",
    "fmri = fmri.to('cuda:0') # needs assignment\n",
    "decoder.to('cuda:0') # \n",
    "imgs = decoder(fmri)\n",
    "imgs = imgs.view(fmri.shape[0], 256, 256)\n",
    "imgs = imgs.data.cpu() * 255.0\n",
    "imgs = np.asarray(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(test_image_list)):\n",
    "    num_0 = 4 - len(str(i))\n",
    "    new = \"0\"*num_0 + str(i)+\".jpg\"\n",
    "    sp = Image.fromarray(imgs[i])\n",
    "    raw = cv2.imread(\"./data/images/images/test/\"+test_image_list[i])\n",
    "    raw  = cv2.resize(np.array(raw), dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "    raw = cv2.cvtColor(raw, cv2.COLOR_BGR2RGB)\n",
    "    raw = Image.fromarray(raw)\n",
    "    target = Image.new('RGB', (256 * 2, 256))\n",
    "    target.paste(raw, (0, 0, 256, 256))\n",
    "    target.paste(sp, (256, 0, 512, 256))\n",
    "    plt.imshow(target)\n",
    "    plt.show()\n",
    "#     break\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
